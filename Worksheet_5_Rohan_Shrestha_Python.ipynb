{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Worksheet - 5**\n",
        "# Name: Rohan Shrestha\n",
        "# ID: 2418112"
      ],
      "metadata": {
        "id": "YdjFcYNHZTEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• To - Do - 1:**\n",
        "1. Read and Observe the Dataset.\n",
        "2. Print top(5) and bottom(5) of the dataset {Hint: pd.head and pd.tail}.\n",
        "3. Print the Information of Datasets. {Hint: pd.info}.\n",
        "4. Gather the Descriptive info about the Dataset. {Hint: pd.describe}\n",
        "5. Split your data into Feature (X) and Label (Y)."
      ],
      "metadata": {
        "id": "xXE5D9EiSrhM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HKzgVkVuOUwm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "data = pd.read_csv('/content/drive/MyDrive/College Materials/Datasets Python/student.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxDuslrCRtQ-",
        "outputId": "9cc22af2-252b-4693-df2b-aca6d0700989"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head())\n",
        "print(data.tail())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w9yEPXOPOuU",
        "outputId": "3fbc5099-2c56-47fd-cc82-2a0b07fe5a29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.info())\n",
        "print(data.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8sXH47rPTif",
        "outputId": "9cc32728-3b9c-4c68-9e56-5202f4716768"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "None\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['Math', 'Reading']].values\n",
        "Y = data['Writing'].values"
      ],
      "metadata": {
        "id": "K8HTKX5cTpSY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To - Do - 2:**\n",
        "1. To make the task easier - let’s assume there is no bias or intercept.\n",
        "2. Create the following matrices:\n",
        "\n",
        "  Y = WTX\n",
        "\n",
        "3. Note: The feature matrix described above does not include a column of 1s, as it assumes the\n",
        "absence of a bias term in the model."
      ],
      "metadata": {
        "id": "xuN5oDw2TaY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To-Do 2: Define Features and Target Matrices\n",
        "X = data[['Math', 'Reading']].values.T  # Transpose to match matrix notation (features x samples)\n",
        "Y = data['Writing'].values.reshape(1, -1)  # Reshape to match matrix notation (1 x samples)\n",
        "\n",
        "# Initialize weights (W) assuming no bias\n",
        "W = np.zeros((X.shape[0], 1))  # 2x1 for two features\n",
        "\n",
        "# Split dataset into training and testing sets (80-20 split)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X.T, Y.T, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "D-RtKkObPWNV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To - Do - 3:**\n",
        "1. Split the dataset into training and test sets.\n",
        "2. You can use an 80-20 or 70-30 split, with 80% (or 70%) of the data used for training and the rest\n",
        "for testing."
      ],
      "metadata": {
        "id": "X6GTZ8W8ZEDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose back for consistency with matrix operations\n",
        "X_train, X_test = X_train.T, X_test.T\n",
        "Y_train, Y_test = Y_train.T, Y_test.T\n",
        "\n",
        "# Shapes of matrices\n",
        "(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape, W.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncFabffAZGGv",
        "outputId": "4b842086-2882-4787-990d-71ff55f3d43d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2, 800), (1, 800), (2, 200), (1, 200), (2, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To - Do:**\n",
        "\n",
        "**We will define a function that:**\n",
        "1. Loads the data and splits it into training and test sets.\n",
        "2. Prepares the feature matrix (X) and target vector (Y).\n",
        "3. Defines the weight matrix (W) and initializes the learning rate and number of iterations.\n",
        "4. Calls the gradient descent function to learn the parameters.\n",
        "5. Evaluates the model using RMSE and R2\n",
        "."
      ],
      "metadata": {
        "id": "GQ76MGUQXt1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(X, Y, W):\n",
        "    \"\"\" Parameters: This function finds the Mean Square Error. Input parameters:\n",
        "    X: Feature Matrix\n",
        "    Y: Target Matrix\n",
        "    W: Weight Matrix\n",
        "    Output Parameters:\n",
        "    cost: accumulated mean square error.\n",
        "    \"\"\"\n",
        "# Your code here:\n",
        "    m = len(Y)\n",
        "    predictions = X @ W  # Predicted values\n",
        "    cost = (1 / (2 * m)) * np.sum((predictions - Y) ** 2)\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "e8BAeUFrPfsw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (m x n).\n",
        "    Y (numpy.ndarray): Target vector (m x 1).\n",
        "    W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "    alpha (float): Learning rate.\n",
        "    iterations (int): Number of iterations for gradient descent.\n",
        "    Returns:\n",
        "    tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values\n",
        "    .\n",
        "    W_update (numpy.ndarray): Updated parameters (n x 1).\n",
        "    cost_history (list): History of cost values over iterations.\n",
        "    \"\"\"\n",
        "    m = len(Y)\n",
        "    cost_history = []\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        predictions = X @ W\n",
        "        loss = predictions - Y\n",
        "        gradient = (1 / m) * (X.T @ loss)\n",
        "        W -= alpha * gradient\n",
        "        cost = cost_function(X, Y, W)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return W, cost_history\n"
      ],
      "metadata": {
        "id": "NuU24lFaPhbT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the Root Mean Squres.\n",
        "    Input Arguments:\n",
        "    Y: Array of actual(Target) Dependent Varaibles.\n",
        "    Y_pred: Array of predeicted Dependent Varaibles.\n",
        "    Output Arguments:\n",
        "    rmse: Root Mean Square.\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.mean((Y - Y_pred) ** 2))\n"
      ],
      "metadata": {
        "id": "jPzEYhADPjhP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the R Squared Error.Input Arguments:\n",
        "    Y: Array of actual(Target) Dependent Varaibles.\n",
        "    Y_pred: Array of predeicted Dependent Varaibles.\n",
        "    Output Arguments:\n",
        "    rsquared: R Squared Error.\n",
        "    \"\"\"\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    ss_tot = np.sum((Y - np.mean(Y)) ** 2)\n",
        "    return 1 - (ss_res / ss_tot)\n"
      ],
      "metadata": {
        "id": "FrVMU2sWPk-I"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    X = data[['Math', 'Reading']].values\n",
        "    Y = data['Writing'].values\n",
        "\n",
        "    # Step 2: Split Data\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 3: Initialize Weights\n",
        "    W = np.zeros(X_train.shape[1])\n",
        "    alpha = 0.01\n",
        "    iterations = 1000\n",
        "\n",
        "    # Step 4: Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Step 5: Predictions\n",
        "    Y_pred = X_test @ W_optimal\n",
        "\n",
        "    # Step 6: Evaluation\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    print(\"Optimal Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10):\", cost_history[:10])\n",
        "    print(\"RMSE:\", model_rmse)\n",
        "    print(\"R-Squared:\", model_r2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBfYnUQ3PnAV",
        "outputId": "1f78f4ff-6342-4347-d5d8-1d4c739e38e6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Weights: [nan nan]\n",
            "Cost History (First 10): [23202998.436207462, 219334644806.26935, 2073340323269641.0, 1.959900179065611e+19, 1.852666766179455e+23, 1.751300491304735e+27, 1.6554803415451996e+31, 1.564902867811581e+35, 1.479281223840523e+39, 1.398344257792357e+43]\n",
            "RMSE: nan\n",
            "R-Squared: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "<ipython-input-6-0842ae30fa8c>:4: RuntimeWarning: overflow encountered in square\n",
            "  cost = (1 / (2 * m)) * np.sum((predictions - Y) ** 2)\n",
            "<ipython-input-7-e3f18dc961d3>:8: RuntimeWarning: overflow encountered in matmul\n",
            "  gradient = (1 / m) * (X.T @ loss)\n",
            "<ipython-input-7-e3f18dc961d3>:9: RuntimeWarning: invalid value encountered in subtract\n",
            "  W -= alpha * gradient\n"
          ]
        }
      ]
    }
  ]
}